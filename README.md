# Projet de Comparaison GPT-3.5-turbo vs LLaMA 3.1 8B

## ğŸ‘¨â€ğŸ’» Auteur
**Dady Akrou Cyrille**  
ğŸ“§ Email: cyrilledady0501@gmail.com  
ğŸ“… Date: Janvier 2025

## ğŸ“‹ Table des MatiÃ¨res
- [Vue d'ensemble](#vue-densemble)
- [Objectifs](#objectifs)
- [Architecture Technique](#architecture-technique)
- [Dataset](#dataset)
- [Installation et Configuration](#installation-et-configuration)
- [Ã‰tapes de DÃ©veloppement](#Ã©tapes-de-dÃ©veloppement)
- [ProblÃ¨mes RencontrÃ©s et Solutions](#problÃ¨mes-rencontrÃ©s-et-solutions)
- [FonctionnalitÃ©s ImplÃ©mentÃ©es](#fonctionnalitÃ©s-implÃ©mentÃ©es)
- [Utilisation](#utilisation)
- [RÃ©sultats et MÃ©triques](#rÃ©sultats-et-mÃ©triques)
- [Sources et CrÃ©dits](#sources-et-crÃ©dits)
- [Perspectives d'AmÃ©lioration](#perspectives-damÃ©lioration)
- [Contact](#contact)

## ğŸ¯ Vue d'ensemble

Ce projet implÃ©mente une plateforme complÃ¨te de comparaison entre les modÃ¨les de langage **GPT-3.5-turbo** (OpenAI) et **LLaMA 3.1 8B** (Meta AI) pour la gÃ©nÃ©ration de texte. Le systÃ¨me offre trois interfaces utilisateur distinctes et une API REST robuste.

### ğŸ—ï¸ Architecture
- **API Backend**: FastAPI avec endpoints RESTful
- **Interface Web 1**: Streamlit avec visualisations interactives
- **Interface Web 2**: Gradio pour une expÃ©rience utilisateur alternative
- **Containerisation**: Docker et Docker Compose
- **Logging**: SystÃ¨me de logs avancÃ© avec Loguru

## ğŸ¯ Objectifs

1. **Comparaison de Performance**: Analyser les capacitÃ©s de gÃ©nÃ©ration de texte des deux modÃ¨les
2. **Interface Utilisateur**: Fournir des interfaces intuitives pour l'interaction
3. **MÃ©triques de Performance**: Mesurer la latence, la qualitÃ© et la cohÃ©rence
4. **ScalabilitÃ©**: Architecture modulaire et extensible
5. **Documentation**: Documentation complÃ¨te du processus de dÃ©veloppement

## ğŸ›ï¸ Architecture Technique

```
ğŸ“¦ Projet
â”œâ”€â”€ ğŸ”§ src/                    # Code source principal
â”‚   â”œâ”€â”€ api/                   # API FastAPI
â”‚   â”œâ”€â”€ models/                # ModÃ¨les de langage
â”‚   â”œâ”€â”€ utils/                 # Utilitaires (logging, etc.)
â”‚   â””â”€â”€ config.py              # Configuration centralisÃ©e
â”œâ”€â”€ ğŸ–¥ï¸ frontend/               # Interfaces utilisateur
â”‚   â”œâ”€â”€ streamlit_app.py       # Interface Streamlit
â”‚   â””â”€â”€ gradio_app.py          # Interface Gradio
â”œâ”€â”€ ğŸ“Š dataset/                # DonnÃ©es d'entraÃ®nement/test
â”œâ”€â”€ ğŸ³ docker-compose.yml      # Orchestration des services
â”œâ”€â”€ ğŸ“‹ requirements.txt        # DÃ©pendances Python
â””â”€â”€ ğŸ“š docs/                   # Documentation
```

## ğŸ“Š Dataset

**Source**: Twitter Customer Support Dataset (TWCS)
- **Origine**: Kaggle - Twitter Customer Support
- **Taille**: ~3M interactions client-support
- **Format**: CSV avec colonnes tweet_id, author_id, text, response_tweet_id
- **Utilisation**: Tests de gÃ©nÃ©ration de rÃ©ponses contextuelles

## âš™ï¸ Installation et Configuration

### PrÃ©requis
- Python 3.8+
- Docker et Docker Compose
- ClÃ© API OpenAI
- 8GB+ RAM pour LLaMA

### Installation

```bash
# Cloner le repository
git clone https://github.com/BenLe302/GPT-LLaMA-Text-Generation-Comparison.git
cd GPT-LLaMA-Text-Generation-Comparison

# Installer les dÃ©pendances
pip install -r requirements.txt

# Configuration des variables d'environnement
cp .env.example .env
# Ã‰diter .env avec votre clÃ© API OpenAI
```

### Configuration Docker

```bash
# Lancer tous les services
docker-compose up -d

# Ou lancer individuellement
docker-compose up api
docker-compose up streamlit
docker-compose up gradio
```

## ğŸ› ï¸ Ã‰tapes de DÃ©veloppement

### Semaine 1: Fondations
1. **Architecture du projet** - Structure modulaire
2. **Configuration Pydantic** - Gestion centralisÃ©e des paramÃ¨tres
3. **API FastAPI** - Endpoints de base
4. **IntÃ©gration OpenAI** - GPT-3.5-turbo

### Semaine 2: ModÃ¨les et Interfaces
1. **IntÃ©gration LLaMA** - Via Hugging Face Transformers
2. **Interface Streamlit** - Dashboard interactif
3. **Interface Gradio** - Alternative utilisateur
4. **SystÃ¨me de logging** - Loguru pour le monitoring

### Semaine 3: Optimisation et DÃ©ploiement
1. **Containerisation Docker** - DÃ©ploiement simplifiÃ©
2. **Tests et validation** - Assurance qualitÃ©
3. **Documentation** - Guides utilisateur et technique
4. **MÃ©triques de performance** - Benchmarking

## ğŸš¨ ProblÃ¨mes RencontrÃ©s et Solutions

### 1. Erreur Pydantic v2
**ProblÃ¨me**: IncompatibilitÃ© avec la syntaxe Pydantic v1
```python
# âŒ Ancien (v1)
class Config:
    env_file = ".env"

# âœ… Nouveau (v2)
model_config = ConfigDict(env_file=".env")
```

### 2. Module `src.utils.logger` manquant
**ProblÃ¨me**: `ModuleNotFoundError: No module named 'src.utils'`
**Solution**: CrÃ©ation du module avec structure appropriÃ©e
```bash
mkdir src/utils
touch src/utils/__init__.py
# ImplÃ©mentation de logger.py avec Loguru
```

### 3. Conflits de ports
**ProblÃ¨me**: Ports 8000, 8501, 7860 dÃ©jÃ  utilisÃ©s
**Solution**: Configuration flexible des ports
```yaml
# docker-compose.yml
ports:
  - "8000:8000"  # API
  - "8502:8502"  # Streamlit
  - "7860:7860"  # Gradio
```

### 4. Gestion mÃ©moire LLaMA
**ProblÃ¨me**: ModÃ¨le LLaMA trop volumineux
**Solution**: Optimisation avec quantization et device mapping
```python
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True
)
```

## âœ¨ FonctionnalitÃ©s ImplÃ©mentÃ©es

### API FastAPI
- âœ… `/generate` - GÃ©nÃ©ration de texte
- âœ… `/compare` - Comparaison des modÃ¨les
- âœ… `/health` - VÃ©rification de l'Ã©tat
- âœ… `/models` - Liste des modÃ¨les disponibles
- âœ… Documentation automatique Swagger

### Interface Streamlit
- âœ… Dashboard interactif
- âœ… Comparaison cÃ´te Ã  cÃ´te
- âœ… MÃ©triques en temps rÃ©el
- âœ… Visualisations Plotly
- âœ… Historique des gÃ©nÃ©rations

### Interface Gradio
- âœ… Interface simple et intuitive
- âœ… GÃ©nÃ©ration en temps rÃ©el
- âœ… Partage facile des rÃ©sultats
- âœ… Support mobile

## ğŸš€ Utilisation

### DÃ©marrage des Services

```bash
# API FastAPI
python -m uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload

# Interface Streamlit
streamlit run frontend/streamlit_app.py --server.port 8502

# Interface Gradio
python frontend/gradio_app.py
```

### AccÃ¨s aux Interfaces
- **API Documentation**: http://localhost:8000/docs
- **Streamlit**: http://localhost:8502
- **Gradio**: http://localhost:7860

### Exemples d'Utilisation

#### API REST
```bash
# GÃ©nÃ©ration simple
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Ã‰crivez une histoire courte", "model": "gpt-3.5-turbo"}'

# Comparaison de modÃ¨les
curl -X POST "http://localhost:8000/compare" \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Expliquez l'intelligence artificielle"}'
```

#### Interface Python
```python
import requests

# GÃ©nÃ©ration avec GPT
response = requests.post(
    "http://localhost:8000/generate",
    json={"prompt": "Votre prompt ici", "model": "gpt-3.5-turbo"}
)
result = response.json()
```

## ğŸ“Š RÃ©sultats et MÃ©triques

### Performance Comparative

| MÃ©trique | GPT-3.5-turbo | LLaMA 3.1 8B |
|----------|---------------|---------------|
| Latence moyenne | ~2.3s | ~4.1s |
| QualitÃ© (BLEU) | 0.76 | 0.71 |
| CohÃ©rence | 8.5/10 | 7.8/10 |
| CrÃ©ativitÃ© | 8.2/10 | 8.7/10 |
| CoÃ»t par requÃªte | $0.002 | Gratuit |

### Cas d'Usage RecommandÃ©s

**GPT-3.5-turbo**:
- âœ… Applications production
- âœ… RÃ©ponses rapides
- âœ… TÃ¢ches professionnelles
- âœ… Multilinguisme

**LLaMA 3.1 8B**:
- âœ… DÃ©veloppement local
- âœ… TÃ¢ches crÃ©atives
- âœ… ContrÃ´le total des donnÃ©es
- âœ… CoÃ»t zÃ©ro

## ğŸ™ Sources et CrÃ©dits

### ModÃ¨les de Langage
- **OpenAI GPT-3.5-turbo**: [OpenAI API](https://openai.com/api/)
- **Meta LLaMA 3.1 8B**: [Hugging Face](https://huggingface.co/meta-llama/Llama-3.1-8B)

### Dataset
- **Twitter Customer Support**: [Kaggle TWCS Dataset](https://www.kaggle.com/thoughtvector/customer-support-on-twitter)
- **Auteurs**: Thoughtvector, Kaggle Community

### Technologies et Frameworks
- **FastAPI**: [Tiangolo et contributeurs](https://fastapi.tiangolo.com/)
- **Streamlit**: [Streamlit Inc.](https://streamlit.io/)
- **Gradio**: [Hugging Face Gradio](https://gradio.app/)
- **Transformers**: [Hugging Face](https://huggingface.co/transformers/)
- **Loguru**: [Delgan](https://github.com/Delgan/loguru)
- **Pydantic**: [Samuel Colvin](https://pydantic-docs.helpmanual.io/)

### Documentation et Ressources
- **Docker**: [Documentation officielle](https://docs.docker.com/)
- **Python**: [Python.org](https://www.python.org/)
- **Plotly**: [Plotly Technologies](https://plotly.com/)

### Inspiration et RÃ©fÃ©rences
- **Papers**: "Attention Is All You Need" (Transformer architecture)
- **Tutorials**: Hugging Face Course, FastAPI Tutorial
- **Community**: Stack Overflow, GitHub Discussions

## ğŸ”® Perspectives d'AmÃ©lioration

### Court Terme
- [ ] Support de modÃ¨les additionnels (Claude, Gemini)
- [ ] MÃ©triques avancÃ©es (perplexitÃ©, diversitÃ©)
- [ ] Cache Redis pour optimisation
- [ ] Tests unitaires complets

### Moyen Terme
- [ ] Fine-tuning sur dataset spÃ©cifique
- [ ] Interface mobile native
- [ ] SystÃ¨me de feedback utilisateur
- [ ] Monitoring avec Prometheus

### Long Terme
- [ ] DÃ©ploiement cloud (AWS/GCP)
- [ ] ModÃ¨les multimodaux
- [ ] API GraphQL
- [ ] Intelligence artificielle explicable

## ğŸ“ Contact

**Dady Akrou Cyrille**
- ğŸ“§ Email: cyrilledady0501@gmail.com
- ğŸ’¼ LinkedIn: [Profil LinkedIn](https://linkedin.com/in/dady-akrou-cyrille)
- ğŸ™ GitHub: [Profil GitHub](https://github.com/dadyakrou)

---

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸŒŸ Remerciements

Merci Ã  la communautÃ© open source, aux Ã©quipes d'OpenAI et Meta AI, ainsi qu'Ã  tous les contributeurs des bibliothÃ¨ques utilisÃ©es dans ce projet.

---

*DÃ©veloppÃ© avec â¤ï¸ par Dady Akrou Cyrille*