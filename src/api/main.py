"""
API FastAPI pour la g√©n√©ration de texte avec GPT et LLaMA

Ce module impl√©mente une API REST compl√®te pour la g√©n√©ration de texte
utilisant les mod√®les GPT-3.5-turbo et LLaMA 3.1 8B.

Auteur: Dady Akrou Cyrille
Email: cyrilledady0501@gmail.com
Date: 2024
"""

import asyncio
import time
import uuid
from datetime import datetime
from typing import Dict, List, Optional, Any, Union

from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field, validator

from src.config import settings
from src.models.model_manager import ModelManager
from src.utils.logger import get_logger

# ============================================================================
# Configuration de l'application
# ============================================================================

app = FastAPI(
    title=settings.PROJECT_NAME,
    description=settings.PROJECT_DESCRIPTION,
    version=settings.PROJECT_VERSION,
    contact={
        "name": settings.PROJECT_AUTHOR,
        "email": settings.PROJECT_EMAIL
    },
    docs_url="/docs" if settings.DEBUG else None,
    redoc_url="/redoc" if settings.DEBUG else None
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Logger
logger = get_logger(__name__)

# Gestionnaire de mod√®les
model_manager = ModelManager()

# S√©curit√© optionnelle
security = HTTPBearer(auto_error=False) if settings.API_REQUIRE_AUTH else None


# ============================================================================
# Mod√®les Pydantic
# ============================================================================

class GenerationRequest(BaseModel):
    """Mod√®le pour les requ√™tes de g√©n√©ration de texte"""
    prompt: str = Field(..., min_length=1, max_length=2000, description="Texte d'entr√©e")
    model_type: str = Field(..., description="Type de mod√®le (gpt ou llama)")
    max_tokens: Optional[int] = Field(None, ge=1, le=4000, description="Nombre maximum de tokens")
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0, description="Temp√©rature de g√©n√©ration")
    system_message: Optional[str] = Field(None, max_length=500, description="Message syst√®me")
    context: Optional[str] = Field(None, max_length=1000, description="Contexte additionnel")
    
    @validator('model_type')
    def validate_model_type(cls, v):
        if v not in ['gpt', 'llama']:
            raise ValueError('model_type doit √™tre "gpt" ou "llama"')
        return v


class ComparisonRequest(BaseModel):
    """Mod√®le pour les requ√™tes de comparaison de mod√®les"""
    prompt: str = Field(..., min_length=1, max_length=2000, description="Texte d'entr√©e")
    models: List[str] = Field(..., description="Liste des mod√®les √† comparer")
    max_tokens: Optional[int] = Field(None, ge=1, le=4000, description="Nombre maximum de tokens")
    temperature: Optional[float] = Field(None, ge=0.0, le=2.0, description="Temp√©rature de g√©n√©ration")
    system_message: Optional[str] = Field(None, max_length=500, description="Message syst√®me")
    
    @validator('models')
    def validate_models(cls, v):
        valid_models = ['gpt', 'llama']
        for model in v:
            if model not in valid_models:
                raise ValueError(f'Mod√®le "{model}" non support√©. Mod√®les valides: {valid_models}')
        if len(v) < 2:
            raise ValueError('Au moins 2 mod√®les requis pour la comparaison')
        return v


class GenerationResult(BaseModel):
    """Mod√®le pour les r√©sultats de g√©n√©ration"""
    generated_text: str
    model_type: str
    generation_time: float
    tokens_used: int
    cost_estimate: float
    metadata: Dict[str, Any] = {}


class GenerationResponse(BaseModel):
    """Mod√®le pour les r√©ponses de g√©n√©ration"""
    request_id: str
    result: GenerationResult
    timestamp: str
    success: bool = True


class ComparisonResponse(BaseModel):
    """Mod√®le pour les r√©ponses de comparaison"""
    request_id: str
    results: Dict[str, Optional[GenerationResult]]
    comparison_metrics: Dict[str, Any]
    best_model: Optional[str]
    timestamp: str
    success: bool = True


class HealthResponse(BaseModel):
    """Mod√®le pour les r√©ponses de sant√©"""
    status: str
    timestamp: str
    version: str
    models_status: Dict[str, str]
    uptime: float


class ErrorResponse(BaseModel):
    """Mod√®le pour les r√©ponses d'erreur"""
    error: str
    error_code: str
    request_id: str
    timestamp: str
    success: bool = False


# ============================================================================
# Utilitaires
# ============================================================================

def generate_request_id() -> str:
    """G√©n√®re un ID unique pour la requ√™te"""
    return str(uuid.uuid4())


def get_current_timestamp() -> str:
    """Retourne le timestamp actuel"""
    return datetime.now().isoformat()


async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Authentification optionnelle"""
    if settings.API_REQUIRE_AUTH and not credentials:
        raise HTTPException(status_code=401, detail="Token d'authentification requis")
    return credentials


# ============================================================================
# Endpoints principaux
# ============================================================================

@app.get("/")
async def root():
    """Endpoint racine"""
    return {
        "message": f"Bienvenue sur {settings.PROJECT_NAME}",
        "author": settings.PROJECT_AUTHOR,
        "email": settings.PROJECT_EMAIL,
        "version": settings.PROJECT_VERSION,
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """V√©rification de l'√©tat de sant√© de l'API"""
    start_time = getattr(app.state, 'start_time', time.time())
    uptime = time.time() - start_time
    
    return HealthResponse(
        status="healthy",
        timestamp=get_current_timestamp(),
        version=settings.PROJECT_VERSION,
        models_status=model_manager.get_model_status(),
        uptime=uptime
    )


@app.post("/generate", response_model=GenerationResponse)
async def generate_text(
    request: GenerationRequest,
    background_tasks: BackgroundTasks,
    current_user=Depends(get_current_user)
):
    """
    G√©n√®re du texte avec le mod√®le sp√©cifi√©
    
    - **prompt**: Texte d'entr√©e pour la g√©n√©ration
    - **model_type**: Type de mod√®le (gpt ou llama)
    - **max_tokens**: Nombre maximum de tokens (optionnel)
    - **temperature**: Temp√©rature de g√©n√©ration (optionnel)
    - **system_message**: Message syst√®me (optionnel)
    - **context**: Contexte additionnel (optionnel)
    """
    request_id = generate_request_id()
    
    try:
        logger.info(f"üîÑ G√©n√©ration d√©marr√©e - ID: {request_id}, Mod√®le: {request.model_type}")
        
        # Pr√©paration des param√®tres
        generation_params = {
            "max_tokens": request.max_tokens or settings.OPENAI_MAX_TOKENS,
            "temperature": request.temperature or settings.OPENAI_TEMPERATURE
        }
        
        # Construction du prompt avec contexte et message syst√®me
        full_prompt = request.prompt
        if request.system_message:
            full_prompt = f"Syst√®me: {request.system_message}\n\nUtilisateur: {full_prompt}"
        if request.context:
            full_prompt = f"Contexte: {request.context}\n\n{full_prompt}"
        
        # G√©n√©ration
        start_time = time.time()
        result = await model_manager.generate_text(
            prompt=full_prompt,
            model_type=request.model_type,
            **generation_params
        )
        generation_time = time.time() - start_time
        
        # Cr√©ation de la r√©ponse
        generation_result = GenerationResult(
            generated_text=result["text"],
            model_type=request.model_type,
            generation_time=generation_time,
            tokens_used=result.get("tokens_used", 0),
            cost_estimate=result.get("cost_estimate", 0.0),
            metadata=result.get("metadata", {})
        )
        
        # Log en arri√®re-plan
        background_tasks.add_task(
            log_generation_success,
            request_id,
            request.model_type,
            generation_time,
            generation_result.tokens_used
        )
        
        return GenerationResponse(
            request_id=request_id,
            result=generation_result,
            timestamp=get_current_timestamp()
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©ration - ID: {request_id}, Erreur: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la g√©n√©ration: {str(e)}"
        )


@app.post("/compare", response_model=ComparisonResponse)
async def compare_models(
    request: ComparisonRequest,
    background_tasks: BackgroundTasks,
    current_user=Depends(get_current_user)
):
    """
    Compare plusieurs mod√®les sur le m√™me prompt
    
    - **prompt**: Texte d'entr√©e pour la g√©n√©ration
    - **models**: Liste des mod√®les √† comparer
    - **max_tokens**: Nombre maximum de tokens (optionnel)
    - **temperature**: Temp√©rature de g√©n√©ration (optionnel)
    - **system_message**: Message syst√®me (optionnel)
    """
    request_id = generate_request_id()
    
    try:
        logger.info(f"üîÑ Comparaison d√©marr√©e - ID: {request_id}, Mod√®les: {request.models}")
        
        # Pr√©paration des param√®tres
        generation_params = {
            "max_tokens": request.max_tokens or settings.OPENAI_MAX_TOKENS,
            "temperature": request.temperature or settings.OPENAI_TEMPERATURE
        }
        
        # Construction du prompt avec message syst√®me
        full_prompt = request.prompt
        if request.system_message:
            full_prompt = f"Syst√®me: {request.system_message}\n\nUtilisateur: {full_prompt}"
        
        # G√©n√©ration parall√®le pour tous les mod√®les
        tasks = []
        for model_type in request.models:
            task = asyncio.create_task(
                generate_with_model(model_type, full_prompt, generation_params)
            )
            tasks.append((model_type, task))
        
        # Attente des r√©sultats
        results = {}
        for model_type, task in tasks:
            try:
                result = await task
                results[model_type] = result
            except Exception as e:
                logger.error(f"‚ùå Erreur mod√®le {model_type}: {e}")
                results[model_type] = None
        
        # Calcul des m√©triques de comparaison
        comparison_metrics = calculate_comparison_metrics(results)
        best_model = determine_best_model(results)
        
        # Log en arri√®re-plan
        background_tasks.add_task(
            log_comparison_success,
            request_id,
            request.models,
            comparison_metrics
        )
        
        return ComparisonResponse(
            request_id=request_id,
            results=results,
            comparison_metrics=comparison_metrics,
            best_model=best_model,
            timestamp=get_current_timestamp()
        )
        
    except Exception as e:
        logger.error(f"‚ùå Erreur comparaison - ID: {request_id}, Erreur: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la comparaison: {str(e)}"
        )


async def generate_with_model(model_type: str, prompt: str, params: Dict[str, Any]) -> GenerationResult:
    """G√©n√®re du texte avec un mod√®le sp√©cifique"""
    start_time = time.time()
    
    result = await model_manager.generate_text(
        prompt=prompt,
        model_type=model_type,
        **params
    )
    
    generation_time = time.time() - start_time
    
    return GenerationResult(
        generated_text=result["text"],
        model_type=model_type,
        generation_time=generation_time,
        tokens_used=result.get("tokens_used", 0),
        cost_estimate=result.get("cost_estimate", 0.0),
        metadata=result.get("metadata", {})
    )


@app.get("/models", response_model=Dict[str, Any])
async def get_models_info():
    """Retourne les informations sur les mod√®les disponibles"""
    return {
        "available_models": ["gpt", "llama"],
        "models_info": model_manager.get_model_info(),
        "default_parameters": {
            "max_tokens": settings.OPENAI_MAX_TOKENS,
            "temperature": settings.OPENAI_TEMPERATURE
        }
    }


@app.post("/models/{model_type}/load")
async def load_model(model_type: str):
    """Charge un mod√®le sp√©cifique"""
    try:
        if model_type not in ["gpt", "llama"]:
            raise HTTPException(status_code=400, detail="Mod√®le non support√©")
        
        model_manager.load_model(model_type)
        return {"message": f"Mod√®le {model_type} charg√© avec succ√®s"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/models/{model_type}/unload")
async def unload_model(model_type: str):
    """D√©charge un mod√®le sp√©cifique"""
    try:
        if model_type not in ["gpt", "llama"]:
            raise HTTPException(status_code=400, detail="Mod√®le non support√©")
        
        model_manager.unload_model(model_type)
        return {"message": f"Mod√®le {model_type} d√©charg√© avec succ√®s"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# Fonctions utilitaires
# ============================================================================

def calculate_comparison_metrics(results: Dict[str, Optional[GenerationResult]]) -> Dict[str, Any]:
    """Calcule les m√©triques de comparaison entre mod√®les"""
    metrics = {
        "total_models": len(results),
        "successful_generations": sum(1 for r in results.values() if r is not None),
        "average_generation_time": 0,
        "total_tokens_used": 0,
        "total_cost_estimate": 0,
        "fastest_model": None,
        "most_efficient_model": None
    }
    
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    if valid_results:
        # Temps moyen
        times = [r.generation_time for r in valid_results.values()]
        metrics["average_generation_time"] = sum(times) / len(times)
        
        # Total tokens et co√ªt
        metrics["total_tokens_used"] = sum(r.tokens_used for r in valid_results.values())
        metrics["total_cost_estimate"] = sum(r.cost_estimate for r in valid_results.values())
        
        # Mod√®le le plus rapide
        fastest = min(valid_results.items(), key=lambda x: x[1].generation_time)
        metrics["fastest_model"] = fastest[0]
        
        # Mod√®le le plus efficace (ratio qualit√©/co√ªt)
        if any(r.cost_estimate > 0 for r in valid_results.values()):
            most_efficient = min(
                valid_results.items(),
                key=lambda x: x[1].cost_estimate / max(x[1].tokens_used, 1)
            )
            metrics["most_efficient_model"] = most_efficient[0]
    
    return metrics


def determine_best_model(results: Dict[str, Optional[GenerationResult]]) -> Optional[str]:
    """D√©termine le meilleur mod√®le bas√© sur plusieurs crit√®res"""
    valid_results = {k: v for k, v in results.items() if v is not None}
    
    if not valid_results:
        return None
    
    # Score composite bas√© sur vitesse et efficacit√©
    scores = {}
    for model_name, result in valid_results.items():
        # Score bas√© sur la vitesse (invers√©)
        speed_score = 1 / max(result.generation_time, 0.1)
        
        # Score bas√© sur l'efficacit√© (tokens/co√ªt)
        if result.cost_estimate > 0:
            efficiency_score = result.tokens_used / result.cost_estimate
        else:
            efficiency_score = result.tokens_used  # LLaMA gratuit
        
        # Score composite
        scores[model_name] = speed_score * 0.3 + efficiency_score * 0.7
    
    return max(scores.items(), key=lambda x: x[1])[0]


async def log_generation_success(request_id: str, model_type: str, generation_time: float, tokens_used: int):
    """Log de succ√®s de g√©n√©ration (t√¢che en arri√®re-plan)"""
    logger.info(f"üìä G√©n√©ration termin√©e - ID: {request_id}, Mod√®le: {model_type}, "
                f"Temps: {generation_time:.2f}s, Tokens: {tokens_used}")


async def log_comparison_success(request_id: str, models: List[str], metrics: Dict[str, Any]):
    """Log de succ√®s de comparaison (t√¢che en arri√®re-plan)"""
    logger.info(f"üìä Comparaison termin√©e - ID: {request_id}, Mod√®les: {models}, "
                f"Meilleur: {metrics.get('fastest_model', 'N/A')}")


# ============================================================================
# Gestionnaire d'erreurs
# ============================================================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    """Gestionnaire d'erreurs HTTP"""
    return JSONResponse(
        status_code=exc.status_code,
        content=ErrorResponse(
            error=exc.detail,
            error_code=f"HTTP_{exc.status_code}",
            request_id=generate_request_id(),
            timestamp=get_current_timestamp()
        ).dict()
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """Gestionnaire d'erreurs g√©n√©rales"""
    logger.error(f"‚ùå Erreur non g√©r√©e: {exc}")
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            error="Erreur interne du serveur",
            error_code="INTERNAL_ERROR",
            request_id=generate_request_id(),
            timestamp=get_current_timestamp()
        ).dict()
    )


# ============================================================================
# √âv√©nements de d√©marrage/arr√™t
# ============================================================================

@app.on_event("startup")
async def startup_event():
    """√âv√©nements au d√©marrage de l'application"""
    logger.info(f"üöÄ D√©marrage de {settings.PROJECT_NAME}")
    logger.info(f"üë®‚Äçüíª D√©velopp√© par {settings.PROJECT_AUTHOR}")
    logger.info(f"üìß Contact: {settings.PROJECT_EMAIL}")
    logger.info(f"üåç Environnement: {settings.ENVIRONMENT}")
    
    # Pr√©chargement du mod√®le GPT (plus rapide)
    try:
        model_manager.load_model("gpt")
        logger.info("‚úÖ Mod√®le GPT pr√©charg√©")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Impossible de pr√©charger GPT: {e}")


@app.on_event("shutdown")
async def shutdown_event():
    """√âv√©nements √† l'arr√™t de l'application"""
    logger.info("üõë Arr√™t de l'application")
    
    # D√©chargement des mod√®les
    try:
        model_manager.unload_all_models()
        logger.info("‚úÖ Tous les mod√®les d√©charg√©s")
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du d√©chargement: {e}")


if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "src.api.main:app",
        host=settings.API_HOST,
        port=settings.API_PORT,
        reload=settings.API_RELOAD,
        workers=1 if settings.DEBUG else settings.API_WORKERS
    )